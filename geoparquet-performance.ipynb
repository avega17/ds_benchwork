{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial file format Performance Evaluation\n",
    "\n",
    "This notebook contains: \n",
    "- An Overview of the GeoParquet standard\n",
    "- Benchmark code for evaluating the GeoParquet file format using the following datasets:\n",
    "    - The [Google-Microsoft combined Open Buildings](https://beta.source.coop/vida/google-microsoft-open-buildings/) for 2D building footprint data\n",
    "    - The [Overture buildings dataset](https://medium.com/mapular/overture-maps-a-fusion-of-open-and-commercial-data-for-a-new-era-in-mapping-f26b4b56ad9a) for 2.5D building data\n",
    "        - Explore the impact of Overture's \n",
    "- A discussion of the current state of cloud-optimized geospatial file formats generally and the potential of GeoParquet specifically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    " GeoParquet is [an incubating Open Geospatial Consortium (OGC) standard](https://geoparquet.org/) that simply adds compatible geospatial [geometry types](https://docs.safe.com/fme/html/FME-Form-Documentation/FME-ReadersWriters/geoparquet/Geometry-Support.htm) (MultiPoint, Line, Polygon, etc) to the mature and widely adopted Apache Parquet format, a popular columnar storage format commonly used in big data processing. Parquet is a mature file format and has a wide ecosystem that GeoParquet seamlessly integrates with. This is analogous to how the GeoTIFF raster format adds geospatial metadata to the TIFF standard. GeoParquet is designed to be a simple and efficient way to store geospatial *vector* data in a columnar format, and is designed to be compatible with existing Parquet tools and libraries to enable Cloud _Data Warehouse_ Interopability. \n",
    "\n",
    "A Parquet file is made up of a a set of file chunks called \"row groups\". Row groups are logical groups of columns with the same number of rows. Each of these columns is actually a \"column chunk\" which is a contiguous block of data for that column. The schema across row groups must be consistent, ie the data types and number of columns must be the same for every row group. The new geospatial standard adds some relevant additional metadata such as the geometry's Coordinate Reference System (CRS), additional metadata for geometry columns, and future realeses will enable support for spatial indexing. [Spatial indexing](https://towardsdatascience.com/geospatial-data-engineering-spatial-indexing-18200ef9160b) is a technique used to optimize spatial queries by indexing or partitioning the data based on its geometry features such that you can make spatial queries (e.g. intersection, within, within x distance, etc) more efficiently. \n",
    "\n",
    "<figure>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*QEQJjtnDb3JQ2xqhzARZZw.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Visualization of the layout of a Parquet file </figcaption>\n",
    "</figure>\n",
    "\n",
    "Beyond the file data itself, Parquet also stores metadata at the end of the file that describes the internal \"chunking\" of the file, byte ranges of every column chunks, several column statistics, among other things. \n",
    "\n",
    "<figure>\n",
    "<img src=\"https://guide.cloudnativegeo.org/images/geoparquet_layout.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> GeoParquet has the same laylout with additional metadata </figcaption>\n",
    "</figure>\n",
    "\n",
    " \n",
    "\n",
    "## Features and Advantages\n",
    "\n",
    "- Efficient storage and compression: \n",
    "    - leverages the columnar data format which is more efficient for filtering on columns\n",
    "    - GeoParquet is internally compressed by default, and can be configured to optimize decompression time or storage size depending on the use case\n",
    "    - These make it ideal for applications dealing with _massive_ geospatial datasets and cloud data warehouses\n",
    "- Scalability and High-Performance:\n",
    "    - the nature of the file format is well-suited for parallel and/or distributed processing such as in Spark, Dask, or Hadoop\n",
    "    - Support for data partitioning: \n",
    "        - Parquet files can be partitioned by one or more columns\n",
    "        - In the geospatial context this enables efficient spatial queries and filtering (e.g. partitioning by ISO country code) \n",
    "- Optimized for *read-heavy workflows*: \n",
    "    - Parquet is an immutable file format, which means taking advantage of cheap reads, and efficient filtering and aggregation operations\n",
    "        - This is ideal for data warehousing and modern analytic workflows \n",
    "        - Best paired with Analytical Databases like Amazon Redshift, Google BigQuery, or DuckDB\n",
    "        - Ideal for OLAP (Online Analytical Processing) and BI (Business Intelligence) workloads that leverage historical and aggregated data that don't require frequent updates\n",
    " - Interoperability and wide ecosystem:\n",
    "    - GeoParquet is designed to be compatible with existing Parquet readers, tools, and libraries\n",
    "    - Facilitates integration into existing data pipelines and workflows\n",
    "    - Broad compatibility:\n",
    "        - support for multiple spatial reference systems \n",
    "        - support for multiple geometry types and multiple geometry columns\n",
    "        - works with both planar and spherical coordinates \n",
    "        - support for 2D and 3D geometries\n",
    "        \n",
    "## Limitations and Disadvantages\n",
    "\n",
    "- Poorly suited for write-heavy workflows:\n",
    "    - Transactional and CRUD (Create, Read, Update, Delete) operations are not well-suited for Parquet files\n",
    "    - Not recommended for applications that require frequent updates or real-time data ingestion\n",
    "- Not a Silver Bullet for all geospatial data:\n",
    "    - deals only with vector data, not raster data\n",
    "    - storage and compression benefits require a certain scale of data to be realized\n",
    "    - performance overhead for small datasets\n",
    "- Limited support for spatial indexing:\n",
    "    - GeoParquet did not implement spatial indexing in the 1.0.0 release\n",
    "    - This is planned for future release in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open buildings data will be saved to:  /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings/parquet\n",
      "Conda prefix:  /Volumes/Expanse/mambaforge/envs/geospatial\n",
      "GDAL driver path:  /Volumes/Expanse/mambaforge/envs/geospatial/lib/gdalplugins\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import platform\n",
    "\n",
    "# Constants and local env configuration\n",
    "load_dotenv() # take environment variables from .env \n",
    "\n",
    "# use filepath loaded from env   \n",
    "open_buildings_path = os.getenv(\"DATA_DIR\") \n",
    "# make sure to parse as raw string to avoid issues with Windows paths\n",
    "open_buildings_path = r\"{}\".format(open_buildings_path)\n",
    "os.makedirs(os.path.join(open_buildings_path, \"parquet\"), exist_ok=True)\n",
    "print(\"Open buildings data will be saved to: \", os.path.join(open_buildings_path, \"parquet\"))\n",
    "\n",
    "# create env vars dict for use in multiprocessing benchmark scripts\n",
    "env_vars = {\n",
    "    \"CONDA_PREFIX\": os.getenv(\"CONDA_PREFIX\"),\n",
    "    \"GDAL_DRIVER_PATH\": os.getenv(\"GDAL_DRIVER_PATH\"),\n",
    "    # add both of these to the PATH so that the GDAL binaries can be found\n",
    "    \"PATH\": os.getenv(\"PATH\") + os.pathsep + os.getenv(\"CONDA_PREFIX\") + os.pathsep + os.getenv(\"GDAL_DRIVER_PATH\")\n",
    "    \n",
    "}\n",
    "\n",
    "print(\"Conda prefix: \", env_vars[\"CONDA_PREFIX\"])\n",
    "print(\"GDAL driver path: \", env_vars[\"GDAL_DRIVER_PATH\"])\n",
    "\n",
    "# list of ISO country codes to fetch \n",
    "# buildings_countries = [\"CUB\",\"CHN\", \"AUS\", \"DEU\" \"USA\"]\n",
    "# buildings_countries = [\"BRB\", \"CAF\", \"JAM\", \"CUB\", \"GTM\", \"AUS\"]\n",
    "buildings_countries = [\"BRB\", \"CAF\", \"JAM\"]\n",
    "file_fmts = [\"geojson\", \"shapefile\", \"flatgeobuf\", \"geopackage\"]\n",
    "compression_types = [\"snappy\", \"gzip\", \"brotli\", None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utilities for fetching and processing data, and benchmarking functions\n",
    "from benchmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAF.parquet found at /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings/parquet/CAF.parquet\n",
      "Testing load time for CAF.parquet\n",
      "BRB.parquet found at /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings/parquet/BRB.parquet\n",
      "Testing load time for BRB.parquet\n",
      "JAM.parquet found at /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings/parquet/JAM.parquet\n",
      "Testing load time for JAM.parquet\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Starting conversion from parquet to geojson\n",
      "Successfully loaded JAM.parquet into geopandas dataframe\n",
      "Starting conversion from parquet to geojson\n",
      "Successfully loaded CAF.parquet into geopandas dataframe\n",
      "Starting conversion from parquet to geojson\n",
      "Successfully converted BRB.parquet to BRB.geojson in 5.34s\n",
      "Converted file size: 100.30 MB\n",
      "Testing load time for BRB.geojson\n",
      "Successfully converted JAM.parquet to JAM.geojson in 28.31s\n",
      "Converted file size: 694.41 MB\n",
      "Testing load time for JAM.geojson\n",
      "Successfully converted CAF.parquet to CAF.geojson in 29.65s\n",
      "Converted file size: 730.72 MB\n",
      "Testing load time for CAF.geojson\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Successfully converted BRB gdf to BRB.geojson in 21.69s\n",
      "geojson file size: 100.30 MB\n",
      "file sizes for geojson - ogr/gdal:100.30 MB, geopandas:100.30 MB\n",
      "Starting conversion from parquet to shapefile\n",
      "Successfully converted BRB.parquet to BRB.shp in 1.46s\n",
      "Converted file size: 34.03 MB\n",
      "Testing load time for BRB.shp\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Expanse/repos/wovenwork/benchwork/ds_benchwork/benchmark.py:156: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  input_df.to_file(output_file, driver=FMT_GDAL_DRIVERS[output_format])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted BRB gdf to BRB.shp in 19.35s\n",
      "shapefile file size: 34.03 MB\n",
      "file sizes for shapefile - ogr/gdal:34.03 MB, geopandas:34.03 MB\n",
      "Starting conversion from parquet to flatgeobuf\n",
      "Successfully converted BRB.parquet to BRB.fgb in 1.08s\n",
      "Converted file size: 56.87 MB\n",
      "Testing load time for BRB.fgb\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Successfully converted BRB gdf to BRB.fgb in 19.78s\n",
      "flatgeobuf file size: 56.87 MB\n",
      "file sizes for flatgeobuf - ogr/gdal:56.87 MB, geopandas:56.87 MB\n",
      "Starting conversion from parquet to geopackage\n",
      "Successfully converted BRB.parquet to BRB.gpkg in 1.20s\n",
      "Converted file size: 56.48 MB\n",
      "Testing load time for BRB.gpkg\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Successfully converted BRB gdf to BRB.gpkg in 28.64s\n",
      "geopackage file size: 55.50 MB\n",
      "file sizes for geopackage - ogr/gdal:56.48 MB, geopandas:55.50 MB\n",
      "Successfully loaded JAM.parquet into geopandas dataframe\n",
      "Successfully loaded CAF.parquet into geopandas dataframe\n",
      "Successfully converted JAM gdf to JAM.geojson in 140.46s\n",
      "geojson file size: 694.41 MB\n",
      "file sizes for geojson - ogr/gdal:694.41 MB, geopandas:694.41 MB\n",
      "Starting conversion from parquet to shapefile\n",
      "Successfully converted JAM.parquet to JAM.shp in 8.93s\n",
      "Converted file size: 233.05 MB\n",
      "Testing load time for JAM.shp\n",
      "Successfully converted CAF gdf to CAF.geojson in 153.97s\n",
      "geojson file size: 730.72 MB\n",
      "file sizes for geojson - ogr/gdal:730.72 MB, geopandas:730.72 MB\n",
      "Starting conversion from parquet to shapefile\n",
      "Successfully converted CAF.parquet to CAF.shp in 9.84s\n",
      "Converted file size: 248.68 MB\n",
      "Testing load time for CAF.shp\n",
      "Successfully loaded JAM.parquet into geopandas dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Expanse/repos/wovenwork/benchwork/ds_benchwork/benchmark.py:156: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  input_df.to_file(output_file, driver=FMT_GDAL_DRIVERS[output_format])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CAF.parquet into geopandas dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Volumes/Expanse/repos/wovenwork/benchwork/ds_benchwork/benchmark.py:156: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  input_df.to_file(output_file, driver=FMT_GDAL_DRIVERS[output_format])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted JAM gdf to JAM.shp in 122.85s\n",
      "shapefile file size: 233.05 MB\n",
      "file sizes for shapefile - ogr/gdal:233.05 MB, geopandas:233.05 MB\n",
      "Starting conversion from parquet to flatgeobuf\n",
      "Successfully converted JAM.parquet to JAM.fgb in 7.38s\n",
      "Converted file size: 392.64 MB\n",
      "Testing load time for JAM.fgb\n",
      "Successfully converted CAF gdf to CAF.shp in 142.06s\n",
      "shapefile file size: 248.68 MB\n",
      "file sizes for shapefile - ogr/gdal:248.68 MB, geopandas:248.68 MB\n",
      "Starting conversion from parquet to flatgeobuf\n",
      "Successfully converted CAF.parquet to CAF.fgb in 8.95s\n",
      "Converted file size: 426.01 MB\n",
      "Testing load time for CAF.fgb\n",
      "Successfully loaded JAM.parquet into geopandas dataframe\n",
      "Successfully loaded CAF.parquet into geopandas dataframe\n",
      "Successfully converted JAM gdf to JAM.fgb in 112.78s\n",
      "flatgeobuf file size: 392.64 MB\n",
      "file sizes for flatgeobuf - ogr/gdal:392.64 MB, geopandas:392.64 MB\n",
      "Starting conversion from parquet to geopackage\n",
      "Successfully converted JAM.parquet to JAM.gpkg in 5.72s\n",
      "Converted file size: 389.33 MB\n",
      "Testing load time for JAM.gpkg\n",
      "Successfully converted CAF gdf to CAF.fgb in 124.96s\n",
      "flatgeobuf file size: 397.06 MB\n",
      "file sizes for flatgeobuf - ogr/gdal:426.01 MB, geopandas:397.06 MB\n",
      "Starting conversion from parquet to geopackage\n",
      "Successfully converted CAF.parquet to CAF.gpkg in 5.95s\n",
      "Converted file size: 425.15 MB\n",
      "Testing load time for CAF.gpkg\n",
      "Successfully loaded JAM.parquet into geopandas dataframe\n",
      "Successfully converted JAM gdf to JAM.gpkg in 124.90s\n",
      "geopackage file size: 382.24 MB\n",
      "file sizes for geopackage - ogr/gdal:389.33 MB, geopandas:382.24 MB\n",
      "Successfully loaded CAF.parquet into geopandas dataframe\n",
      "Successfully converted CAF gdf to CAF.gpkg in 133.11s\n",
      "geopackage file size: 416.89 MB\n",
      "file sizes for geopackage - ogr/gdal:425.15 MB, geopandas:416.89 MB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# run full benchmarking pipeline\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m conversion_stats, compression_stats \u001b[38;5;241m=\u001b[39m \u001b[43mfull_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcountry_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuildings_countries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mfile_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_fmts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mcompression_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopen_buildings_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mdelete_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mtest_load\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43menv_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43mmulti_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                        \u001b[49m\u001b[43msave_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Expanse/repos/wovenwork/benchwork/ds_benchwork/benchmark.py:366\u001b[0m, in \u001b[0;36mfull_benchmark\u001b[0;34m(country_list, file_formats, compression_types, data_dir, delete_output, test_load, env_vars, multi_proc, save_results)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_results:\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# concatenate country codes as part of filename\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     convert_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(country_list) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_conversion_benchmark.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 366\u001b[0m     \u001b[43msave_benchmark_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconversion_stats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    367\u001b[0m     compress_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(country_list) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_compression_benchmark.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    368\u001b[0m     save_benchmark_stats(compression_stats, compress_filename)\n",
      "File \u001b[0;32m/Volumes/Expanse/repos/wovenwork/benchwork/ds_benchwork/benchmark.py:317\u001b[0m, in \u001b[0;36msave_benchmark_stats\u001b[0;34m(df, output_file)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_benchmark_stats\u001b[39m(df, output_file):\n\u001b[0;32m--> 317\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(output_file, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved benchmark stats with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetsize(output_file)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1024\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m KB)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# run full benchmarking pipeline\n",
    "conversion_stats, compression_stats = full_benchmark(country_list=buildings_countries,\n",
    "                                                        file_formats=file_fmts,\n",
    "                                                        compression_types=compression_types,\n",
    "                                                        data_dir=open_buildings_path,\n",
    "                                                        delete_output=True,\n",
    "                                                        test_load=True, \n",
    "                                                        env_vars=env_vars,\n",
    "                                                        multi_proc=True,\n",
    "                                                        save_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_stats = [\"processing_time\", \"file_size\", \"load_time\"]\n",
    "convert_stats_df = flatten_benchmark_stats(stats=conversion_stats, column_name=\"output_format\", stats_names=convert_stats)\n",
    "\n",
    "compress_stats = [\"compression_time\", \"compression_size\", \"load_time\", \"geom_count\"]\n",
    "compress_stats_df = flatten_benchmark_stats(stats=compression_stats, column_name=\"compression_type\", stats_names=compress_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # in output format column, change \"parquet\" to \"geoparquet\"\n",
    "    convert_stats_df[\"output_format\"] = convert_stats_df[\"output_format\"].apply(lambda x: \"geoparquet\" if x == \"parquet\" else x)\n",
    "    # add geom_count column to convert_stats_df by joining on country_code\n",
    "    convert_stats_df = convert_stats_df.merge(compress_stats_df[[\"country_code\", \"geom_count\"]], on=\"country_code\")\n",
    "    # display updated convert_stats_df  \n",
    "    convert_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_stats_df.name = \"Conversion Stats\"\n",
    "# save to csv with country_codes in filename\n",
    "country_codes = \"_\".join(convert_stats_df[\"country_code\"].unique())\n",
    "convert_stats_df.to_csv(f\"{open_buildings_path}/benchmark_{country_codes}_conversion.csv\", index=False)\n",
    "pretty_print_df_info(convert_stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for plotting stats with seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def barplot(data, x, y, title, xlabel, ylabel, hue=None, orient='v', color=None, xticks=None, yticks=None, xticklabels=None, yticklabels=None):\n",
    "    # create horizontal barplot\n",
    "    ax = sns.barplot(data=data, x=x, y=y, hue=hue, orient=orient, color=color)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    if xticks is not None:\n",
    "        ax.set_xticks(xticks)\n",
    "    if yticks is not None:\n",
    "        ax.set_yticks(yticks)    \n",
    "    if xticklabels is not None:\n",
    "        ax.set_xticklabels(xticklabels)\n",
    "    if yticklabels is not None:\n",
    "        ax.set_yticklabels(yticklabels)\n",
    "\n",
    "def scatter_plot(data, x, y, title, xlabel, ylabel, hue=None, xticks=None, yticks=None, xticklabels=None, yticklabels=None):\n",
    "    # create scatter plot\n",
    "    ax = sns.scatterplot(data=data, x=x, y=y, hue=hue)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    if xticks is not None:\n",
    "        ax.set_xticklabels(xticks)\n",
    "    if yticks is not None:\n",
    "        ax.set_yticklabels(yticks)\n",
    "    if xticklabels is not None:\n",
    "        ax.set_xticklabels(xticklabels)\n",
    "    if yticklabels is not None:\n",
    "        ax.set_yticklabels(yticklabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grouped bar plot where y is file size and x is file format and file sizes are grouped by country code\n",
    "barplot(data=convert_stats_df, x=\"output_format\", y=\"file_size\", title=\"File size by output format\", xlabel=\"Output format\", ylabel=\"File size (MB)\", hue=\"country_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grouped bar plot where y is load time and x is file format and load times are grouped by country code\n",
    "barplot(data=convert_stats_df, x=\"output_format\", y=\"load_time\", title=\"Load time by output format\", xlabel=\"Output format\", ylabel=\"Load time (s)\", hue=\"country_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check df columns types\n",
    "convert_stats_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify xticks for geom_count to display 200K ticks up to max geom_count in dataset\n",
    "xticks = [i for i in range(0, max(convert_stats_df[\"geom_count\"]), 200000)]\n",
    "\n",
    "print()\n",
    "# modify yticks for load_time to display 30s ticks up to max load_time in dataset\n",
    "yticks = [f\"{i}s\" for i in range(0, max(convert_stats_df[\"load_time\"].astype(int)), 30)]\n",
    "\n",
    "# create a scatter plot where x is geom count and y is load time and points are colored by file format\n",
    "scatter_plot(data=convert_stats_df, x=\"geom_count\", y=\"load_time\", title=\"Load time by geom count\", xlabel=\"Geom count\", ylabel=\"Load time (s)\", hue=\"output_format\", xticks=xticks, yticks=yticks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying in-memory with GeoPandas \n",
    "\n",
    "Query brainstorming: \n",
    "- simple geometry/building count per country\n",
    "    - visualization: interactive international map with countries shaded or scaled by building count\n",
    "- distribution of square area of buildings per country or per other attribute\n",
    "    - comparison of square area distributions between countries\n",
    "        - how do we compare/match distributions? descriptive statistics? \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying from files with DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Basemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Data with Overture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion on cloud-native geospatial data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://geoparquet.org/\n",
    "- https://geopandas.org/\n",
    "- https://radiant.earth/blog/2023/10/what-is-source-cooperative/\n",
    "- https://guide.cloudnativegeo.org/geoparquet/\n",
    "- https://medium.com/mapular/overture-maps-a-fusion-of-open-and-commercial-data-for-a-new-era-in-mapping-f26b4b56ad9a\n",
    "- https://towardsdatascience.com/geospatial-data-engineering-spatial-indexing-18200ef9160b\n",
    "- https://github.com/opengeospatial/geoparquet/blob/main/format-specs/geoparquet.md \n",
    "- https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
