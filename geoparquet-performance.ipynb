{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geospatial file format Performance Evaluation\n",
    "\n",
    "This notebook contains: \n",
    "- An Overview of the GeoParquet standard\n",
    "- Benchmark code for evaluating the GeoParquet file format using the following datasets:\n",
    "    - The [Google-Microsoft combined Open Buildings](https://beta.source.coop/vida/google-microsoft-open-buildings/) for 2D building footprint data\n",
    "    - The [Overture buildings dataset](https://medium.com/mapular/overture-maps-a-fusion-of-open-and-commercial-data-for-a-new-era-in-mapping-f26b4b56ad9a) for 2.5D building data\n",
    "        - Explore the impact of Overture's \n",
    "- A discussion of the current state of cloud-optimized geospatial file formats generally and the potential of GeoParquet specifically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    " GeoParquet is [an incubating Open Geospatial Consortium (OGC) standard](https://geoparquet.org/) that simply adds compatible geospatial [geometry types](https://docs.safe.com/fme/html/FME-Form-Documentation/FME-ReadersWriters/geoparquet/Geometry-Support.htm) (MultiPoint, Line, Polygon, etc) to the mature and widely adopted Apache Parquet format, a popular columnar storage format commonly used in big data processing. Parquet is a mature file format and has a wide ecosystem that GeoParquet seamlessly integrates with. This is analogous to how the GeoTIFF raster format adds geospatial metadata to the TIFF standard. GeoParquet is designed to be a simple and efficient way to store geospatial *vector* data in a columnar format, and is designed to be compatible with existing Parquet tools and libraries to enable Cloud _Data Warehouse_ Interopability. \n",
    "\n",
    "A Parquet file is made up of a a set of file chunks called \"row groups\". Row groups are logical groups of columns with the same number of rows. Each of these columns is actually a \"column chunk\" which is a contiguous block of data for that column. The schema across row groups must be consistent, ie the data types and number of columns must be the same for every row group. The new geospatial standard adds some relevant additional metadata such as the geometry's Coordinate Reference System (CRS), additional metadata for geometry columns, and future realeses will enable support for spatial indexing. [Spatial indexing](https://towardsdatascience.com/geospatial-data-engineering-spatial-indexing-18200ef9160b) is a technique used to optimize spatial queries by indexing or partitioning the data based on its geometry features such that you can make spatial queries (e.g. intersection, within, within x distance, etc) more efficiently. \n",
    "\n",
    "<figure>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*QEQJjtnDb3JQ2xqhzARZZw.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Visualization of the layout of a Parquet file </figcaption>\n",
    "</figure>\n",
    "\n",
    "Beyond the file data itself, Parquet also stores metadata at the end of the file that describes the internal \"chunking\" of the file, byte ranges of every column chunks, several column statistics, among other things. \n",
    "\n",
    "<figure>\n",
    "<img src=\"https://guide.cloudnativegeo.org/images/geoparquet_layout.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> GeoParquet has the same laylout with additional metadata </figcaption>\n",
    "</figure>\n",
    "\n",
    " \n",
    "\n",
    "## Features and Advantages\n",
    "\n",
    "- Efficient storage and compression: \n",
    "    - leverages the columnar data format which is more efficient for filtering on columns\n",
    "    - GeoParquet is internally compressed by default, and can be configured to optimize decompression time or storage size depending on the use case\n",
    "    - These make it ideal for applications dealing with _massive_ geospatial datasets and cloud data warehouses\n",
    "- Scalability and High-Performance:\n",
    "    - the nature of the file format is well-suited for parallel and/or distributed processing such as in Spark, Dask, or Hadoop\n",
    "    - Support for data partitioning: \n",
    "        - Parquet files can be partitioned by one or more columns\n",
    "        - In the geospatial context this enables efficient spatial queries and filtering (e.g. partitioning by ISO country code) \n",
    "- Optimized for *read-heavy workflows*: \n",
    "    - Parquet is an immutable file format, which means taking advantage of cheap reads, and efficient filtering and aggregation operations\n",
    "        - This is ideal for data warehousing and modern analytic workflows \n",
    "        - Best paired with Analytical Databases like Amazon Redshift, Google BigQuery, or DuckDB\n",
    "        - Ideal for OLAP (Online Analytical Processing) and BI (Business Intelligence) workloads that leverage historical and aggregated data that don't require frequent updates\n",
    " - Interoperability and wide ecosystem:\n",
    "    - GeoParquet is designed to be compatible with existing Parquet readers, tools, and libraries\n",
    "    - Facilitates integration into existing data pipelines and workflows\n",
    "    - Broad compatibility:\n",
    "        - support for multiple spatial reference systems \n",
    "        - support for multiple geometry types and multiple geometry columns\n",
    "        - works with both planar and spherical coordinates \n",
    "        - support for 2D and 3D geometries\n",
    "        \n",
    "## Limitations and Disadvantages\n",
    "\n",
    "- Poorly suited for write-heavy workflows:\n",
    "    - Transactional and CRUD (Create, Read, Update, Delete) operations are not well-suited for Parquet files\n",
    "    - Not recommended for applications that require frequent updates or real-time data ingestion\n",
    "- Not a Silver Bullet for all geospatial data:\n",
    "    - deals only with vector data, not raster data\n",
    "    - storage and compression benefits require a certain scale of data to be realized\n",
    "    - performance overhead for small datasets\n",
    "- Limited support for spatial indexing:\n",
    "    - GeoParquet did not implement spatial indexing in the 1.0.0 release\n",
    "    - This is planned for future release in "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open buildings data will be saved to:  /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Constants and local env configuration\n",
    "load_dotenv() # take environment variables from .env \n",
    "\n",
    "# URI for combined open buildings data\n",
    "combined_open_buildings_uri = \"https://data.source.coop/vida/google-microsoft-open-buildings/geoparquet/by_country\"\n",
    "\n",
    "# use filepath loaded from env   \n",
    "open_buildings_path = os.getenv(\"DATA_DIR\") \n",
    "print(\"Open buildings data will be saved to: \", open_buildings_path)\n",
    "# list of ISO country codes to fetch \n",
    "# buildings_countries = [\"CUB\",\"CHN\", \"AUS\", \"DEU\" \"USA\"]\n",
    "buildings_countries = [\"BRB\", \"CUB\", \"JAM\"]\n",
    "file_fmt_map = {\"geojson\":\".geojson\", \"shapefile\":\".shp\", \"flatgeobuf\":\".fgb\", \"geopackage\":\".gpkg\"}\n",
    "format_gdal_names = {\"geojson\":\"GeoJSON\", \"shapefile\":\"ESRI Shapefile\", \"flatgeobuf\":\"FlatGeobuf\", \"geopackage\":\"GPKG\"}\n",
    "compression_types = [\"snappy\", \"gzip\", \"brotli\", None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filesystem performance tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "# import time library to show download time\n",
    "import time\n",
    "import os\n",
    "from glob import glob\n",
    "import subprocess\n",
    "\n",
    "def download_with_progress(url, local_file_path):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    total_size = int(response.getheader('Content-Length').strip())\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "\n",
    "    with open(local_file_path, 'wb') as file, tqdm(\n",
    "        desc=local_file_path,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as bar:\n",
    "        while True:\n",
    "            buffer = response.read(block_size)\n",
    "            if not buffer:\n",
    "                break\n",
    "            file.write(buffer)\n",
    "            bar.update(len(buffer))\n",
    "\n",
    "def fetch_geoparquet(country_code, output_dir):\n",
    "    \n",
    "    # fetch geoparquet from source.coop\n",
    "    url = f\"{combined_open_buildings_uri}/country_iso={country_code}/{country_code}.parquet\"\n",
    "    output_file = f\"{output_dir}/{country_code}.parquet\"\n",
    "    print(f\"Downloading {country_code}.parquet from:\\n {url}\")\n",
    "    t1 = time.time()\n",
    "    download_with_progress(url, output_file)\n",
    "    t2 = time.time()\n",
    "    print(f\"Succesfully downloaded {country_code}.parquet in {t2-t1} seconds and saved to {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def load_geodataframe(input_file, country_code, output_format=\"parquet\", test_load=False):\n",
    "\n",
    "    load_time = None\n",
    "    # load geopandas dataframe from geoparquet\n",
    "    if output_format == \"parquet\":\n",
    "        # use timeit magic method to measure time taken to load\n",
    "        if test_load:\n",
    "            print(f\"Testing load time for {country_code}.parquet\")\n",
    "            # get timeItResult object\n",
    "            load_time = %timeit -r 1 -n 1 -o gpd.read_parquet(input_file)\n",
    "            # save median of all_runs \n",
    "            load_time = np.median(load_time.all_runs)\n",
    "        input_df = gpd.read_parquet(input_file)\n",
    "    # load geopandas dataframe from other formats\n",
    "    else:\n",
    "        if test_load:\n",
    "            print(f\"Testing load time for {input_file.split('/')[-1]}\")\n",
    "            # get timeItResult object\n",
    "            load_time = %timeit -r 1 -n 1 -o gpd.read_file(input_file)\n",
    "            # save median of all_runs \n",
    "            load_time = np.median(load_time.all_runs)\n",
    "        input_df = gpd.read_file(input_file)\n",
    "    input_df.name = country_code\n",
    "    print(f\"Successfully loaded {country_code}.parquet into geopandas dataframe\")\n",
    "    \n",
    "    return input_df, load_time\n",
    "\n",
    "def pretty_print_df_info(input_df):\n",
    "    print(f\"Dataframe info for {input_df.name}:\")\n",
    "    print(input_df.info())\n",
    "    print(input_df.head())\n",
    "\n",
    "def get_output_path(input_file, output_format):\n",
    "    # output path is same as input, but up two \n",
    "    # input has form /path/to/data/format/country_code.format\n",
    "    country_code = input_file.split(\"/\")[-1].split(\".\")[0] # use same country code for output file name\n",
    "    output_file = \"/\".join(input_file.split(\"/\")[:-2]) \n",
    "    # output file format is /path/to/data/format/country_code.format\n",
    "    output_file = f\"{output_file}/{output_format}/{country_code}{file_fmt_map[output_format]}\"\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ogr2ogr to convert geoparquet to our target formats; delete output file and return time taken and file size\n",
    "def ogr_gdal_convert(input_file, output_format, delete_output=True, test_load=False):\n",
    "\n",
    "    output_file = get_output_path(input_file, output_format)\n",
    "    convert_time = time.time()\n",
    "    file_size = 0\n",
    "    load_time = None\n",
    "    # get gdal format name\n",
    "    input_format = input_file.split(\".\")[-1] # get file extension\n",
    "    print(f\"Starting conversion from {input_format} to {output_format}\")\n",
    "    gdal_format = format_gdal_names[output_format]\n",
    "    command = [\"ogr2ogr\", \"-f\", gdal_format, output_file, input_file]\n",
    "    result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    t2 = time.time()\n",
    "    convert_time = t2 - convert_time\n",
    "    \n",
    "    # verify conversion result\n",
    "    if result.returncode == 0:\n",
    "        # calculate file size of converted file and convert to MB\n",
    "        file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "        print(f\"Successfully converted {input_file.split('/')[-1]} to {output_file.split('/')[-1]} in {convert_time:.2f}s\")\n",
    "        print(f\"Converted file size: {file_size:.2f} MB\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error in conversion: {result.stderr.decode()}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if test_load:\n",
    "        # load geopandas dataframe from converted file\n",
    "        country_code = input_file.split(\"/\")[-1].split(\".\")[0] \n",
    "        _, load_time = load_geodataframe(input_file=output_file, country_code=country_code, output_format=output_format, test_load=True)\n",
    "\n",
    "    # delete output file\n",
    "    if delete_output:\n",
    "        # delete other files like in the case of shapefile\n",
    "        for file in glob(f\"{output_file.split('.')[0]}*\"):\n",
    "            os.remove(file)\n",
    "        \n",
    "    return convert_time, file_size, load_time\n",
    "\n",
    "# use geopandas to save to our target formats from existing dataframe; delete output file and return time taken and file size\n",
    "def geopandas_convert(input_df, output_format, delete_output=True):\n",
    "    \n",
    "    # get country_code from input_df\n",
    "    output_file = f\"{open_buildings_path}/{output_format}/{input_df.name}{file_fmt_map[output_format]}\"\n",
    "    convert_time = time.time()\n",
    "    load_time = None\n",
    "    \n",
    "    try:\n",
    "        input_df.to_file(output_file, driver=format_gdal_names[output_format])\n",
    "        t2 = time.time()\n",
    "        convert_time = t2 - convert_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {input_df} to {output_format}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # calculate file size of converted file and convert to MB\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    # delete output file\n",
    "    if delete_output:\n",
    "        # delete other files like in the case of shapefile\n",
    "        for file in glob(f\"{output_file.split('.')[0]}*\"):\n",
    "            os.remove(file) \n",
    "            \n",
    "    print(f\"Successfully converted {input_df.name} gdf to {output_file.split('/')[-1]} in {convert_time:.2f}s\")\n",
    "    print(f\"{output_format} file size: {file_size:.2f} MB\")\n",
    "\n",
    "    return convert_time, file_size\n",
    "\n",
    "# use duckdb to read in geoparquet and save to our target formats\n",
    "def duckdb_convert(input_file, output_format):\n",
    "    output_file = get_output_path(input_file, output_format)\n",
    "\n",
    "# receive geopandas df, save file to disk using specified algorithm, delete output file and return time taken and file size\n",
    "def gdf_to_compressed_geoparquet(input_df, compression_type, test_load=False):\n",
    "    \n",
    "    output_file = f\"{open_buildings_path}/parquet/{input_df.name}_{compression_type}.parquet\"\n",
    "    convert_time = time.time()\n",
    "    load_time = None\n",
    "\n",
    "    try:\n",
    "        input_df.to_parquet(output_file, compression=compression_type, schema_version='1.0.0')\n",
    "        t2 = time.time()\n",
    "        convert_time = t2 - convert_time\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {input_df.name} gdf to geoparquet compressed with {compression_type}: {e}\")\n",
    "    \n",
    "    # calculate file size of converted file and convert to MB\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    \n",
    "    if test_load:\n",
    "        print(f\"Testing load time for {input_df.name}_{compression_type}.parquet\")\n",
    "        # load geopandas dataframe from converted file\n",
    "        _, load_time = load_geodataframe(input_file=output_file, country_code=input_df.name, output_format=\"parquet\", test_load=True)\n",
    "    \n",
    "    \n",
    "    # delete output file\n",
    "    os.remove(output_file)\n",
    "    print(f\"Successfully saved {input_df.name} gdf to geoparquet compressed with {compression_type} in {convert_time} seconds.\")\n",
    "    print(f\"Converted file size: {file_size} MB\")    \n",
    "\n",
    "    return convert_time, file_size, load_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# benchmark performance of converting to other vector formats \n",
    "\n",
    "def convert_benchmark(country_list, file_formats, data_dir, delete_output, test_load):\n",
    "    \n",
    "    # initialize stats for conversion\n",
    "    convert_stats = {\n",
    "            \"processing_time\": [], # for different conversion methods (ogr2ogr, geopandas, duckdb)\n",
    "            \"file_size\": 0.0, \n",
    "            \"load_time\": 0.0 # time taken to load converted file into geopandas dataframe\n",
    "    }\n",
    "    conversion_stats = {}    \n",
    "    \n",
    "    # go through each country code, fetch geoparquet, convert to target formats, and records some stats\n",
    "    for country_code in country_list:\n",
    "        \n",
    "        # download geoparquet for each benchmarked country if not already downloaded\n",
    "        input_file = f\"{data_dir}/parquet/{country_code}.parquet\"\n",
    "        if not os.path.exists(f\"{data_dir}/parquet/{country_code}.parquet\"):\n",
    "            print(f\"{country_code}.parquet not found, fetching from source.coop...\")\n",
    "            input_file = fetch_geoparquet(country_code, f\"{data_dir}/parquet\")\n",
    "        else:\n",
    "            print(f\"{country_code}.parquet found at {input_file}\")\n",
    "        conversion_stats[country_code] = {}\n",
    "\n",
    "        # calculate processing time and file size for geoparquet \n",
    "        parquet_file_size = os.path.getsize(input_file) / (1024 * 1024)\n",
    "        country_gdf, load_time = load_geodataframe(input_file, country_code, output_format=\"parquet\", test_load=test_load)\n",
    "        conversion_stats[country_code][\"parquet\"] = {\"file_size\": parquet_file_size, \"processing_time\": [0.0, 0.0], \"load_time\": load_time}\n",
    "        \n",
    "        for output_format in file_formats:\n",
    "            # create output dir for each file format if it doesn't exist\n",
    "            output_dir = f\"{data_dir}/{output_format}\"\n",
    "            if not os.path.exists(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            # initialize stats for country_code and output_format\n",
    "            conversion_stats[country_code][output_format] = convert_stats.copy() \n",
    "            # gdal/ogr2ogr conversion\n",
    "            ogr_time, ogr_size, load_time = ogr_gdal_convert(input_file, output_format, delete_output, test_load)\n",
    "            # geopandas conversion\n",
    "            gpd_time, gpd_size = geopandas_convert(country_gdf, output_format, delete_output)\n",
    "            # duckdb conversion\n",
    "            # TODO: implement duckdb conversion\n",
    "            \n",
    "            # record stats\n",
    "            conversion_stats[country_code][output_format][\"processing_time\"] = [ogr_time, gpd_time]\n",
    "            # file size for a given format should be mostly the same, so we take the max\n",
    "            conversion_stats[country_code][output_format][\"file_size\"] = max(ogr_size, gpd_size)\n",
    "            conversion_stats[country_code][output_format][\"load_time\"] = load_time\n",
    "            # format to two decimal places\n",
    "            print(f\"file sizes for {output_format} - ogr/gdal:{ogr_size:.2f} MB, geopandas:{gpd_size:.2f} MB\")\n",
    "    \n",
    "    return conversion_stats\n",
    "        \n",
    " # benchmark compression performance of geoparquet\n",
    " \n",
    "def compress_benchmark(country_list, compression_types, data_dir, delete_output, test_load):\n",
    "     \n",
    "    # initialize stats for compression\n",
    "    compress_stats = {\n",
    "                        \"compression_size\": 0.0,\n",
    "                        \"compression_time\": 0.0,\n",
    "                        \"load_time\": 0.0, \n",
    "                        \"geom_count\": 0\n",
    "    }\n",
    "    compression_stats = {}\n",
    "    # go through each country code, fetch geoparquet, convert to target formats, and records some stats\n",
    "    for country_code in country_list:\n",
    "    \n",
    "        # download geoparquet for each benchmarked country if not already downloaded\n",
    "        input_file = f\"{data_dir}/parquet/{country_code}.parquet\"\n",
    "        if not os.path.exists(f\"{open_buildings_path}/parquet/{country_code}.parquet\"):\n",
    "            print(f\"{country_code}.parquet not found, fetching from source.coop...\")\n",
    "            input_file = fetch_geoparquet(country_code, f\"{data_dir}/parquet\")\n",
    "        else:\n",
    "            print(f\"{country_code}.parquet found at {input_file}\")\n",
    "    \n",
    "        country_gdf, _ = load_geodataframe(input_file, country_code)\n",
    "        geom_count = len(country_gdf)\n",
    "        compression_stats[country_code] = {}\n",
    "\n",
    "        # compress geopandas dataframe\n",
    "        for ctype in compression_types:\n",
    "            \n",
    "            ctype = \"None\" if ctype is None else ctype\n",
    "            compression_stats[country_code][ctype] = compress_stats.copy()\n",
    "            compress_time, compress_size, load_time = gdf_to_compressed_geoparquet(country_gdf, ctype, test_load=test_load)\n",
    "            compression_stats[country_code][ctype][\"compression_time\"] = compress_time\n",
    "            compression_stats[country_code][ctype][\"compression_size\"] = compress_size\n",
    "            compression_stats[country_code][ctype][\"load_time\"] = load_time\n",
    "            compression_stats[country_code][ctype][\"geom_count\"] = geom_count\n",
    "        \n",
    "    return compression_stats\n",
    "\n",
    "def flatten_benchmark_stats(stats, column_name, stats_names):\n",
    "    \n",
    "    # both dicts have the same two levels of keys (country_code and output_format/compression_type)\n",
    "    # flatten dict into single level of columns\n",
    "    data = []\n",
    "    for country_code, country_stats in stats.items():\n",
    "        # print(country_code, country_stats)\n",
    "        for key, stats in country_stats.items():\n",
    "            # print(key, stats)\n",
    "            row = {\"country_code\": country_code, column_name: key}\n",
    "            for stat_name in stats_names:\n",
    "                row[stat_name] = stats[stat_name]\n",
    "            data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "    \n",
    "\n",
    "# full benchmarking pipeline\n",
    "def full_benchmark(country_list, file_formats, compression_types, data_dir, delete_output, test_load):\n",
    "    \n",
    "    print(\"Testing conversion performance...\")\n",
    "    conversion_stats = convert_benchmark(country_list, file_formats, data_dir, delete_output, test_load)\n",
    "    print(\"Testing compression performance...\")\n",
    "    compression_stats = compress_benchmark(country_list, compression_types, data_dir, delete_output, test_load)\n",
    "    \n",
    "    return conversion_stats, compression_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing conversion performance...\n",
      "BRB.parquet found at /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings/parquet/BRB.parquet\n",
      "Testing load time for BRB.parquet\n",
      "254 ms ± 69.5 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Starting conversion from parquet to geojson\n",
      "Successfully converted BRB.parquet to BRB.geojson in 3.98s\n",
      "Converted file size: 100.30 MB\n",
      "Testing load time for BRB.geojson\n",
      "17.8 s ± 121 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Successfully converted BRB gdf to BRB.geojson in 19.71s\n",
      "geojson file size: 100.30 MB\n",
      "file sizes for geojson - ogr/gdal:100.30 MB, geopandas:100.30 MB\n",
      "Starting conversion from parquet to shapefile\n",
      "Successfully converted BRB.parquet to BRB.shp in 1.23s\n",
      "Converted file size: 34.03 MB\n",
      "Testing load time for BRB.shp\n",
      "14 s ± 594 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/71/3fjqkxbs6w3frshvsfj30fmm0000gn/T/ipykernel_57140/1918500258.py:50: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  input_df.to_file(output_file, driver=format_gdal_names[output_format])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted BRB gdf to BRB.shp in 16.56s\n",
      "shapefile file size: 34.03 MB\n",
      "file sizes for shapefile - ogr/gdal:34.03 MB, geopandas:34.03 MB\n",
      "Starting conversion from parquet to flatgeobuf\n",
      "Successfully converted BRB.parquet to BRB.fgb in 0.76s\n",
      "Converted file size: 56.87 MB\n",
      "Testing load time for BRB.fgb\n",
      "13.7 s ± 637 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Successfully converted BRB gdf to BRB.fgb in 15.99s\n",
      "flatgeobuf file size: 56.87 MB\n",
      "file sizes for flatgeobuf - ogr/gdal:56.87 MB, geopandas:56.87 MB\n",
      "Starting conversion from parquet to geopackage\n",
      "Successfully converted BRB.parquet to BRB.gpkg in 0.87s\n",
      "Converted file size: 56.48 MB\n",
      "Testing load time for BRB.gpkg\n",
      "13.4 s ± 92.9 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded BRB.parquet into geopandas dataframe\n",
      "Successfully converted BRB gdf to BRB.gpkg in 26.36s\n",
      "geopackage file size: 55.50 MB\n",
      "file sizes for geopackage - ogr/gdal:56.48 MB, geopandas:55.50 MB\n",
      "CUB.parquet found at /Volumes/NAS_Base/nas_data/geospatial/google-ms-open-buildings/parquet/CUB.parquet\n",
      "Testing load time for CUB.parquet\n",
      "4.72 s ± 882 ms per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded CUB.parquet into geopandas dataframe\n",
      "Starting conversion from parquet to geojson\n",
      "Successfully converted CUB.parquet to CUB.geojson in 77.95s\n",
      "Converted file size: 2189.45 MB\n",
      "Testing load time for CUB.geojson\n",
      "6min 36s ± 4.37 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n",
      "Successfully loaded CUB.parquet into geopandas dataframe\n",
      "Successfully converted CUB gdf to CUB.geojson in 442.22s\n",
      "geojson file size: 2189.45 MB\n",
      "file sizes for geojson - ogr/gdal:2189.45 MB, geopandas:2189.45 MB\n",
      "Starting conversion from parquet to shapefile\n",
      "Successfully converted CUB.parquet to CUB.shp in 26.62s\n",
      "Converted file size: 727.98 MB\n",
      "Testing load time for CUB.shp\n",
      "5min 15s ± 2.03 s per loop (mean ± std. dev. of 3 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# run full benchmarking pipeline\n",
    "conversion_stats, compression_stats = full_benchmark(country_list=buildings_countries,\n",
    "                                                        file_formats=list(file_fmt_map.keys()),\n",
    "                                                        compression_types=compression_types,\n",
    "                                                        data_dir=open_buildings_path,\n",
    "                                                        delete_output=True,\n",
    "                                                        test_load=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_stats = [\"processing_time\", \"file_size\", \"load_time\"]\n",
    "convert_stats_df = flatten_benchmark_stats(stats=conversion_stats, column_name=\"output_format\", stats_names=convert_stats)\n",
    "\n",
    "compress_stats = [\"compression_time\", \"compression_size\", \"load_time\", \"geom_count\"]\n",
    "compress_stats_df = flatten_benchmark_stats(stats=compression_stats, column_name=\"compression_type\", stats_names=compress_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pretty print df by sampling one of the downloaded parquets, load into gdf and print info\n",
    "import random\n",
    "random_country = random.choice(os.listdir(parquet_path))\n",
    "random_df = load_geodataframe(f\"{parquet_path}/{random_country}\", random_country.split(\".\")[0])\n",
    "pretty_print_df_info(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for plotting stats with seaborn\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def barplot(data, x, y, title, xlabel, ylabel, hue=None, orient='v', color=None):\n",
    "    # create horizontal barplot\n",
    "    ax = sns.barplot(data=data, x=x, y=y, hue=hue, orient=orient, color=color)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    # modify container values format\n",
    "    # ax.bar_label(ax.containers[0], fmt= '%0.1f' )\n",
    "\n",
    "def scatter_plot(data, x, y, title, xlabel, ylabel, hue=None):\n",
    "    # create scatter plot\n",
    "    ax = sns.scatterplot(data=data, x=x, y=y, hue=hue)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grouped bar plot where y is file size and x is file format and file sizes are grouped by country code\n",
    "barplot(data=convert_stats_df, x=\"output_format\", y=\"file_size\", title=\"File size by output format\", xlabel=\"Output format\", ylabel=\"File size (MB)\", hue=\"country_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying in-memory with GeoPandas \n",
    "\n",
    "Query brainstorming: \n",
    "- simple geometry/building count per country\n",
    "    - visualization: interactive international map with countries shaded or scaled by building count\n",
    "- distribution of square area of buildings per country or per other attribute\n",
    "    - comparison of square area distributions between countries\n",
    "        - how do we compare/match distributions? descriptive statistics? \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying from files with DuckDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Basemaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Data with Overture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion on cloud-native geospatial data formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- https://geoparquet.org/\n",
    "- https://geopandas.org/\n",
    "- https://radiant.earth/blog/2023/10/what-is-source-cooperative/\n",
    "- https://guide.cloudnativegeo.org/geoparquet/\n",
    "- https://medium.com/mapular/overture-maps-a-fusion-of-open-and-commercial-data-for-a-new-era-in-mapping-f26b4b56ad9a\n",
    "- https://towardsdatascience.com/geospatial-data-engineering-spatial-indexing-18200ef9160b\n",
    "- https://github.com/opengeospatial/geoparquet/blob/main/format-specs/geoparquet.md \n",
    "- https://medium.com/radiant-earth-insights/geoparquet-1-1-coming-soon-9b72c900fbf2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
